{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team KASA\n",
    "\n",
    "\n",
    "\n",
    "#### Extracting, cropping and preprocessing images from SVHN dataset\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook contains the code to extract SVHN dataset, crop the images by specified image size (eg. 32x32 or 54x54) and depth (rgb=3, grey_scale=1) and stored as a .h5 file extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing packages if not previously installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install seaborn\n",
    "# ! pip install h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display, Image, HTML\n",
    "import h5py\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16.0, 4.0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading datasets and extracting images\n",
    "The datasets are downloaded from <a href=\"http://ufldl.stanford.edu/housenumbers/\">this page</a>.    \n",
    "There are distributed into three categories- train, test and extra. Depending on memory constraints, the required datsets can be downloaded.  \n",
    "Each of these catergories contains images as well as a digitStruct.mat that specifies the boundaries of the image to be extracted   \n",
    "**NOTE:** The extra dataset contains ~ 2 lakh images and one should ensure there is enough memory to download this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **** CHANGE !! **** Select the datafiles to be loaded   \n",
    " Since uploading extra takes a lot of time the code has been commented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train tar file....\n",
      "train tar file loaded!\n",
      "Loading test tar file....\n",
      "test tar file loaded!\n",
      "Extracting train.tar.gz ...\n",
      "Extracting test.tar.gz ...\n"
     ]
    }
   ],
   "source": [
    "# **** CHANGE *** Select the datafiles to be loaded\n",
    "# datasets=['train','test','extra']\n",
    "datasets=['train','test']\n",
    "\n",
    "from data_import import load_file\n",
    "load_file(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping images using digitStruct.mat  and generating the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This may take a while!!\n",
      "Collecting digit structure for train data...\n",
      "Collecting digit structure for test data...\n"
     ]
    }
   ],
   "source": [
    "from extract_box import DigitStructWrapper\n",
    "\n",
    "print(\"This may take a while!!\")\n",
    "print('Collecting digit structure for train data...')\n",
    "digitFile=DigitStructWrapper(os.path.join('data','train','digitStruct.mat'))\n",
    "box_train=digitFile.unpack_all()\n",
    "\n",
    "print('Collecting digit structure for test data...')\n",
    "digitFile=DigitStructWrapper(os.path.join('data','test','digitStruct.mat'))\n",
    "box_test=digitFile.unpack_all()\n",
    "\n",
    "# print('Collecting digit structure for extra data...')\n",
    "# digitFile=DigitStructWrapper(os.path.join('data','extra','digitStruct.mat'))\n",
    "# box_extra=digitFile.unpack_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Train (33402, 32, 32, 3) (33402, 6)\n",
      "Test set\n",
      "Test (13068, 32, 32, 3) (13068, 6)\n"
     ]
    }
   ],
   "source": [
    "size_image=32\n",
    "depth=3\n",
    "\n",
    "from crop_generate_data import building_dataset\n",
    "print('Train set')\n",
    "X_train, y_train = building_dataset(box_train, \"data/train/\",size_image,depth)\n",
    "print(\"Train\", X_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "print('Test set')\n",
    "X_test, y_test = building_dataset(box_test, \"data/test/\",size_image,depth)\n",
    "print(\"Test\", X_test.shape, y_test.shape)\n",
    "\n",
    "# print('Extra set')\n",
    "# X_extra, y_extra = building_dataset(box_extra, \"data/extra/\",size_image,depth=)\n",
    "# print(\"Test\", X_extra.shape, y_extra.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the restrict to 5-digit numbers instead of 6-digit numbers run the below code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (33402, 32, 32, 3) (33402, 5)\n",
      "Test set (13068, 32, 32, 3) (13068, 5)\n"
     ]
    }
   ],
   "source": [
    "y_train=y_train[:,:-1]\n",
    "# y_extra=y_extra[:,:-1]\n",
    "y_test=y_test[:,:-1]\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "# print('Extra set', X_val.shape, y_extra.shape)\n",
    "print('Test set', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert to greyscale image from rgb run the below code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depth=1\n",
    "def rgb2gray(images):\n",
    "    \"\"\"Convert images from rbg to grayscale\n",
    "    \"\"\"\n",
    "    greyscale = np.dot(images, [0.2989, 0.5870, 0.1140])\n",
    "    return np.expand_dims(greyscale, axis=3)\n",
    "\n",
    "\n",
    "# Transform the images to greyscale\n",
    "X_train = rgb2gray(X_train).astype(np.float32)\n",
    "X_test = rgb2gray(X_test).astype(np.float32)\n",
    "# X_extra = rgb2gray(X_extra).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (33402, 32, 32, 1) (33402, 5)\n",
      "Test set (13068, 32, 32, 1) (13068, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Training set', X_train.shape, y_train.shape)\n",
    "# print('Extra set', X_val.shape, y_extra.shape)\n",
    "print('Test set', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split to train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (29402, 32, 32, 1) (29402, 5)\n",
      "Validation (4000, 32, 32, 1) (4000, 5)\n"
     ]
    }
   ],
   "source": [
    "def random_sample(a, b):\n",
    "    patch = np.array([True]*a + [False]*(a-b))\n",
    "    np.random.shuffle(patch)\n",
    "    return patch\n",
    "\n",
    "# Pick 4000 training and 2000 extra samples\n",
    "sample1 = random_sample(X_train.shape[0], 4000)\n",
    "# sample2 = random_sample(X_extra.shape[0], 2000)\n",
    "\n",
    "# Create valdidation from the sampled data\n",
    "X_val = np.concatenate([X_train[sample1]])\n",
    "y_val = np.concatenate([y_train[sample1]])\n",
    "\n",
    "# Keep the data not contained by sample\n",
    "X_train = np.concatenate([X_train[~sample1]])\n",
    "y_train = np.concatenate([y_train[~sample1]])\n",
    "\n",
    "# Moved to validation and training set\n",
    "# del X_extra, y_extra \n",
    "\n",
    "print(\"Training\", X_train.shape, y_train.shape)\n",
    "print('Validation', X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset will be stored in the data folder by the name given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'digits_32_32_1.h5'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_str=\"digits_\"+str(size_image)+\"_\"+str(size_image)+\"_\"+str(depth)+\".h5\"\n",
    "file_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file\n",
    "h5f = h5py.File('data/'+file_str, 'w')\n",
    "\n",
    "# Store the datasets\n",
    "h5f.create_dataset('train_dataset', data=X_train)\n",
    "h5f.create_dataset('train_labels', data=y_train)\n",
    "h5f.create_dataset('valid_dataset', data=X_val)\n",
    "h5f.create_dataset('valid_labels', data=y_val)\n",
    "\n",
    "# Close the file\n",
    "h5f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
